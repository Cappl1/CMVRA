{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bas06400/Thesis/VIP/src/multimodal_dataset.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(self.mean).view(-1, 1, 1)  # Adjust shape for broadcasting\n",
      "/home/bas06400/Thesis/VIP/src/multimodal_dataset.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(self.std).view(-1, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([1, 224, 224]) 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from multimodal_dataset import SingleFrameVideoDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)  # Seed for Python's random module\n",
    "torch.manual_seed(seed)  # Seed for PyTorch random number generators\n",
    "\n",
    "data_root = '/net/polaris/storage/deeplearning/ntu'\n",
    "data_list = '/home/bas06400/Thesis/rgb_ir_dataset.txt'\n",
    "# Initialize the single frame dataset\n",
    "single_frame_data = SingleFrameVideoDataset(data_list, data_root, ['rgb','ir'], use_advanced_processing=True)\n",
    "\n",
    "# Let's check the shape for the single frame dataset\n",
    "print(single_frame_data[0][0]['rgb'].shape, single_frame_data[0][0]['ir'].shape, single_frame_data[0][1])\n",
    "\n",
    "# Calculate lengths of splits\n",
    "total_len = len(single_frame_data)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len = int(0.1 * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "# Split the single frame dataset\n",
    "train_data, val_data, test_data = random_split(single_frame_data, [train_len, val_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches of data from MultiModalVideoDataset.\n",
    "    \n",
    "    Args:\n",
    "    - batch (list): List of samples fetched from `MultiModalVideoDataset`.\n",
    "    \n",
    "    Returns:\n",
    "    - collated_data (dict): Collated data for each modality.\n",
    "    - collated_labels (tensor): Collated labels.\n",
    "    \"\"\"\n",
    "    collated_data = {}\n",
    "    collated_labels = []\n",
    "    collated_index = []\n",
    "    \n",
    "    # Initialize empty lists for each modality in the first sample\n",
    "    for modality in batch[0][0].keys():\n",
    "        collated_data[modality] = []\n",
    "    \n",
    "    for data, label in batch:\n",
    "        collated_labels.append(label-1)\n",
    "        for modality, frames in data.items():\n",
    "            collated_data[modality].append(frames)\n",
    "        \n",
    "    # Convert lists to tensors for each modality\n",
    "    for modality, frames_list in collated_data.items():\n",
    "        collated_data[modality] = torch.stack(frames_list)\n",
    "    \n",
    "    collated_labels = torch.tensor(collated_labels)\n",
    "    \n",
    "    return collated_data, collated_labels\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,  \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,  \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "from transformers.models.clip.configuration_clip import CLIPConfig\n",
    "\n",
    "# Load the pre-trained CLIP model and tokenizer\n",
    "clip_model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(clip_model_name).to('cuda:3')\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "\n",
    "text_descriptions = [\n",
    "    \"drink water.\",\n",
    "    \"eat meal/snack.\",\n",
    "    \"brushing teeth.\",\n",
    "    \"brushing hair.\",\n",
    "    \"drop.\",\n",
    "    \"pickup.\",\n",
    "    \"throw.\",\n",
    "    \"sitting down.\",\n",
    "    \"standing up (from sitting position).\",\n",
    "    \"clapping.\",\n",
    "    \"reading.\",\n",
    "    \"writing.\",\n",
    "    \"tear up paper.\",\n",
    "    \"wear jacket.\",\n",
    "    \"take off jacket.\",\n",
    "    \"wear a shoe.\",\n",
    "    \"take off a shoe.\",\n",
    "    \"wear on glasses.\",\n",
    "    \"take off glasses.\",\n",
    "    \"put on a hat/cap.\",\n",
    "    \"take off a hat/cap.\",\n",
    "    \"cheer up.\",\n",
    "    \"hand waving.\",\n",
    "    \"kicking something.\",\n",
    "    \"reach into pocket.\",\n",
    "    \"hopping (one foot jumping).\",\n",
    "    \"jump up.\",\n",
    "    \"make a phone call/answer phone.\",\n",
    "    \"playing with phone/tablet.\",\n",
    "    \"typing on a keyboard.\",\n",
    "    \"pointing to something with finger.\",\n",
    "    \"taking a selfie.\",\n",
    "    \"check time (from watch).\",\n",
    "    \"rub two hands together.\",\n",
    "    \"nod head/bow.\",\n",
    "    \"shake head.\",\n",
    "    \"wipe face.\",\n",
    "    \"salute.\",\n",
    "    \"put the palms together.\",\n",
    "    \"cross hands in front (say stop).\",\n",
    "    \"sneeze/cough.\",\n",
    "    \"staggering.\",\n",
    "    \"falling.\",\n",
    "    \"touch head (headache).\",\n",
    "    \"touch chest (stomachache/heart pain).\",\n",
    "    \"touch back (backache).\",\n",
    "    \"touch neck (neckache).\",\n",
    "    \"nausea or vomiting condition.\",\n",
    "    \"use a fan (with hand or paper)/feeling warm.\",\n",
    "    \"punching/slapping other person.\",\n",
    "    \"kicking other person.\",\n",
    "    \"pushing other person.\",\n",
    "    \"pat on back of other person.\",\n",
    "    \"point finger at the other person.\",\n",
    "    \"hugging other person.\",\n",
    "    \"giving something to other person.\",\n",
    "    \"touch other person's pocket.\",\n",
    "    \"handshaking.\",\n",
    "    \"walking towards each other.\",\n",
    "    \"walking apart from each other.\"\n",
    "]\n",
    "\n",
    "# Tokenize the text descriptions\n",
    "text_inputs = tokenizer(text_descriptions, return_tensors=\"pt\", padding=True, truncation=True).to('cuda:3')\n",
    "\n",
    "# Obtain text embeddings using the text model of CLIP\n",
    "with torch.no_grad():\n",
    "    text_outputs = model.get_text_features(**text_inputs)\n",
    "    text_embeddings = text_outputs\n",
    "\n",
    "print(text_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████| 89/89 [06:38<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Accuracy: 2.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_cosine_similarity(x1, x2):\n",
    "    # x1 has shape (batch_size, embed_dim)\n",
    "    # x2 has shape (num_text_descriptions, embed_dim)\n",
    "    dot = x1 @ x2.T\n",
    "    norm1 = torch.norm(x1, p=2, dim=1).unsqueeze(1)\n",
    "    norm2 = torch.norm(x2, p=2, dim=1).unsqueeze(0)\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "correct_rgb_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Assume text_embeddings are computed outside this code block and are on the same device\n",
    "text_embeddings = text_embeddings.to('cuda:3')\n",
    "\n",
    "# This loop assumes that test_loader is defined and provides batches of (rgb_data, batch_labels)\n",
    "for batch_data, batch_labels in tqdm(test_loader, desc=\"Evaluating\", ncols=100):\n",
    "    # Move data to the appropriate device\n",
    "    rgb_data = batch_data['rgb'].to('cuda:3')\n",
    "    batch_labels = batch_labels.to('cuda:3')\n",
    "\n",
    "    model.eval()\n",
    "    # Extract embeddings for the RGB data from the model\n",
    "    rgb_emb = model.get_image_features(rgb_data)\n",
    "    \n",
    "    # Compute cosine similarities for the RGB embeddings\n",
    "    similarities_rgb = batch_cosine_similarity(rgb_emb, text_embeddings)\n",
    "    \n",
    "    # Get predicted classes by finding the index of the max similarity\n",
    "    predicted_class_rgb = torch.argmax(similarities_rgb, dim=1)\n",
    "    \n",
    "    # Update correct predictions count\n",
    "    correct_rgb_predictions += (predicted_class_rgb == batch_labels).sum().item()\n",
    "    \n",
    "    # Update total samples count\n",
    "    total_samples += batch_labels.size(0)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_rgb = correct_rgb_predictions / total_samples\n",
    "print(f\"RGB Accuracy: {accuracy_rgb * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_remote_kernel",
   "language": "python",
   "name": "my_remote_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
