{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.VidCLIP import VidCLIP\n",
    "from easydict import EasyDict as edict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_unique_samples(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Reads from an input JSONL file, extracts unique samples based on the description,\n",
    "    and writes these unique samples to an output JSONL file.\n",
    "    \n",
    "    :param input_file_path: Path to the input JSONL file.\n",
    "    :param output_file_path: Path where the output JSONL file will be saved.\n",
    "    \"\"\"\n",
    "    unique_descriptions = {}\n",
    "    \n",
    "    # Read from the input JSONL file\n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            item = json.loads(line)\n",
    "            if item['text'] not in unique_descriptions:\n",
    "                unique_descriptions[item['text']] = item['clip_id']\n",
    "    \n",
    "    # Write to the output JSONL file\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for text, clip_id in unique_descriptions.items():\n",
    "            unique_sample = json.dumps({'clip_id': clip_id, 'text': text})\n",
    "            output_file.write(unique_sample + '\\n')\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = '/home/bas06400/Thesis/VIP/src/developmentANDtest/annotations_rgb_comp_CV_testing_120set.jsonl'  # You need to replace this with the actual input file path\n",
    "output_file_path = 'unique_samples120test.jsonl'  # You need to replace this with the desired output file path\n",
    "\n",
    "# Call the function with the paths to your files\n",
    "extract_unique_samples(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPModel were not initialized from the model checkpoint at openai/clip-vit-base-patch16 and are newly initialized: ['vision_model.embeddings.temporal_embedding', 'vision_model.embeddings.added_cls']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VidCLIP(\n",
      "  (clipmodel): CLIPModel(\n",
      "    (text_model): CLIPTextTransformer(\n",
      "      (embeddings): CLIPTextEmbeddings(\n",
      "        (token_embedding): Embedding(49408, 512)\n",
      "        (position_embedding): Embedding(77, 512)\n",
      "      )\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionViPEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "        (position_embedding): Embedding(197, 768)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an 'args' object from the provided JSON structure\n",
    "args = edict({\n",
    "    \"clip_config\": \"openai/clip-vit-base-patch16\",\n",
    "    \"clip_weights\": \"openai/clip-vit-base-patch16\",\n",
    "    \"clip_vision_additional_config\": edict({\n",
    "        \"type\": \"ViP\",\n",
    "        \"temporal_size\": 12,\n",
    "        \"if_use_temporal_embed\": True,\n",
    "        \"logit_scale_init_value\": 4.60,\n",
    "        \"add_cls_num\": 3\n",
    "    }),\n",
    "    \"e2e_weights_path\": \"path/to/CLIP-ViP-B/16/checkpoint\"\n",
    "})\n",
    "\n",
    "# Initialize the model instance\n",
    "model_instance = VidCLIP(args)\n",
    "print(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('/home/bas06400/Thesis/pretrain_clipvip_base_16.pt')\n",
    "model_instance.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionViPEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling.CLIP_ViP import CLIPVisionModel, CLIPVisionTransformer, CLIPTextModel, CLIPTextTransformer\n",
    "from transformers.models.clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n",
    "from transformers import CLIPPreTrainedModel\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Given json_config\n",
    "json_config = {\n",
    "    # ... (rest of your JSON config)\n",
    "    \"additional_vision_config\": {\n",
    "        \"type\": \"ViP\",\n",
    "        \"temporal_size\": 12,\n",
    "        \"if_use_temporal_embed\": 1,\n",
    "        \"logit_scale_init_value\": 4.60,\n",
    "        \"add_cls_num\": 3,\n",
    "        \"hiiden_size\": 12\n",
    "    },\n",
    "    # ... (rest of your JSON config)\n",
    "}\n",
    "class SimpleNamespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    \n",
    "# Load the base CLIPConfig\n",
    "clipconfig = CLIPVisionConfig.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "additional_vision_config_obj = SimpleNamespace(**json_config[\"additional_vision_config\"])\n",
    "setattr(clipconfig, \"additional_vision_config\", additional_vision_config_obj)\n",
    "class CLIPVisionModel(CLIPPreTrainedModel):\n",
    "    config_class = CLIPVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__(config)\n",
    "        # Pass the additional_vision_config to CLIPVisionTransformer\n",
    "        self.vision_model = CLIPVisionTransformer(config, config.additional_vision_config)\n",
    "        # Add the visual projection layer\n",
    "        self.visual_projection = nn.Linear(768, 512, bias=False)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, pixel_values, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        # Get the output from the vision_model\n",
    "        vision_output = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        pooled_output = vision_output[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        \n",
    "        return image_features\n",
    "\n",
    "model = CLIPVisionModel(clipconfig)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vidclip_model is the instance of your VidCLIP model\n",
    "vidclip_model_weights = model_instance.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "# Prepare a dictionary to hold the relevant weights\n",
    "state_dict = {}\n",
    "\n",
    "# Copy weights from the vidclip_model_weights to state_dict\n",
    "for name, param in vidclip_model_weights.items():\n",
    "    if \"vision_model\" in name:\n",
    "        new_name = name.replace(\"clipmodel.\", \"\")  # remove the prefix\n",
    "        state_dict[new_name] = param\n",
    "    if \"visual_projection\" in name:\n",
    "        new_name = name.replace(\"clipmodel.\", \"\")  # remove the prefix\n",
    "        state_dict[new_name] = param\n",
    "\n",
    "# Load the state_dict into clip_vision_model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_dataset import MultiModalVideoDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224]) torch.Size([12, 1, 224, 224]) 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)  # Seed for Python's random module\n",
    "torch.manual_seed(seed)  # Seed for PyTorch random number generators\n",
    "\n",
    "\n",
    "data_root = '/net/polaris/storage/deeplearning/ntu'\n",
    "data_list = '/home/bas06400/Thesis/rgb_ir_dataset.txt'\n",
    "data = MultiModalVideoDataset(data_list, data_root, ['rgb','ir'], use_advanced_processing=True)\n",
    "\n",
    "print(data[0][0]['rgb'].shape, data[0][0]['ir'].shape, data[0][1])\n",
    "\n",
    "# Calculate lengths of splits\n",
    "total_len = len(data)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len = int(0.1 * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(data, [train_len, val_len, test_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor batch_data, batch_labels in test_loader:\\n    \\n    print(batch_data['rgb'].shape,batch_data['ir'].shape)\\n    break\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches of data from MultiModalVideoDataset.\n",
    "    \n",
    "    Args:\n",
    "    - batch (list): List of samples fetched from `MultiModalVideoDataset`.\n",
    "    \n",
    "    Returns:\n",
    "    - collated_data (dict): Collated data for each modality.\n",
    "    - collated_labels (tensor): Collated labels.\n",
    "    \"\"\"\n",
    "    collated_data = {}\n",
    "    collated_labels = []\n",
    "    collated_index = []\n",
    "    \n",
    "    # Initialize empty lists for each modality in the first sample\n",
    "    for modality in batch[0][0].keys():\n",
    "        collated_data[modality] = []\n",
    "    \n",
    "    for data, label, idx in batch:\n",
    "        collated_labels.append(label-1)\n",
    "        for modality, frames in data.items():\n",
    "            collated_data[modality].append(frames)\n",
    "        collated_index.append(idx)\n",
    "    # Convert lists to tensors for each modality\n",
    "    for modality, frames_list in collated_data.items():\n",
    "        collated_data[modality] = torch.stack(frames_list)\n",
    "    \n",
    "    collated_labels = torch.tensor(collated_labels)\n",
    "    \n",
    "    return collated_data, collated_labels, collated_index\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 8\n",
    "shuffle = True\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,  \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,  \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\"\"\"\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    \n",
    "    print(batch_data['rgb'].shape,batch_data['ir'].shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionViPEmbeddings(\n",
       "      (patch_embedding): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_model = CLIPVisionModel(clipconfig)\n",
    "# Load the state_dict into clip_vision_model\n",
    "ir_model.load_state_dict(state_dict)\n",
    "\n",
    "ir_model.vision_model.embeddings.patch_embedding = nn.Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
    "\"\"\"\n",
    "# Now, average the weights across the RGB channels\n",
    "old_weights = state_dict['vision_model.embeddings.patch_embedding.weight']\n",
    "new_weights = old_weights.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Assign the averaged weights to the new patch_embedding layer\n",
    "ir_model.vision_model.embeddings.patch_embedding.weight.data = new_weights\n",
    "\"\"\"\n",
    "\n",
    "state_dict = torch.load(\"/home/bas06400/Thesis/best_ir_encoder.pth\")\n",
    "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "ir_model.load_state_dict(state_dict)\n",
    "\n",
    "ir_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom tqdm import tqdm\\n\\nmodel = torch.nn.DataParallel(model, device_ids=[0,1,2, 3]).cuda()  # Assuming GPUs 3 and 4 are available\\nir_model = torch.nn.DataParallel(ir_model, device_ids=[0,1, 2, 3]).cuda()\\n# Hyperparameters\\nlearning_rate = 0.001\\nnum_epochs = 20\\ntemperature = 0.07  # Temperature parameter for InfoNCE loss\\n\\n# Initialize the optimizer\\noptimizer = optim.Adam(ir_model.parameters(), lr=learning_rate)\\n\\n# InfoNCE Loss function\\ndef info_nce_loss(emb1, emb2, temperature=0.07):\\n    # Compute similarity matrix\\n    sim_matrix = torch.mm(emb1, emb2.t())\\n    # Scale similarity by temperature\\n    sim_matrix = sim_matrix / temperature\\n    # Calculate loss\\n    loss = F.cross_entropy(sim_matrix, torch.arange(sim_matrix.size(0)).to(emb1.device))\\n    return loss\\n\\n# Placeholder for best validation loss\\nbest_val_loss = float(\\'inf\\')\\n\\n# Training loop\\nfor epoch in range(num_epochs):\\n    epoch_loss = 0.0\\n    model.train()\\n    ir_model.train()\\n    # Wrap dataloader with tqdm for progress bar\\n    for batch_data, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\\n        # Move data to GPU\\n        rgb_data = batch_data[\\'rgb\\'].cuda() #.to(\\'cuda:3\\')\\n        ir_data = batch_data[\\'ir\\'].cuda() #.to(\\'cuda:3\\')\\n\\n        # Zero the gradients\\n        optimizer.zero_grad()\\n\\n        # Forward pass: Get embeddings or representations from model\\n        rgb_emb  = model(rgb_data)\\n        ir_emb  = ir_model(ir_data)\\n\\n        # Compute the contrastive loss\\n        loss = info_nce_loss(rgb_emb, ir_emb, temperature)\\n\\n        # Backward pass\\n        loss.backward()\\n\\n        # Update weights\\n        optimizer.step()\\n\\n        epoch_loss += loss.item()\\n        #break\\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {epoch_loss / len(train_loader):.4f}\")\\n    # Validation loop\\n    model.eval()\\n    ir_model.eval()\\n    with torch.no_grad():\\n        val_loss = 0.0\\n        for batch_data, _ in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\\n            rgb_data = batch_data[\\'rgb\\'].cuda() #.to(\\'cuda:3\\')\\n            ir_data = batch_data[\\'ir\\'].cuda() #.to(\\'cuda:3\\')\\n            rgb_emb  = model(rgb_data)\\n            ir_emb  = ir_model(ir_data)\\n            loss = info_nce_loss(rgb_emb, ir_emb, temperature)\\n            val_loss += loss.item()\\n            #break\\n        avg_val_loss = val_loss / len(val_loader)\\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\\n        \\n        # Save the best model (optional)\\n        if avg_val_loss < best_val_loss:\\n            best_val_loss = avg_val_loss\\n            torch.save(model.state_dict(), \\'best_ir_encoder.pth\\')\\n\\nprint(\"Training complete!\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1,2, 3]).cuda()  # Assuming GPUs 3 and 4 are available\n",
    "ir_model = torch.nn.DataParallel(ir_model, device_ids=[0,1, 2, 3]).cuda()\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "temperature = 0.07  # Temperature parameter for InfoNCE loss\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(ir_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# InfoNCE Loss function\n",
    "def info_nce_loss(emb1, emb2, temperature=0.07):\n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = torch.mm(emb1, emb2.t())\n",
    "    # Scale similarity by temperature\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(sim_matrix, torch.arange(sim_matrix.size(0)).to(emb1.device))\n",
    "    return loss\n",
    "\n",
    "# Placeholder for best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    ir_model.train()\n",
    "    # Wrap dataloader with tqdm for progress bar\n",
    "    for batch_data, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to GPU\n",
    "        rgb_data = batch_data['rgb'].cuda() #.to('cuda:3')\n",
    "        ir_data = batch_data['ir'].cuda() #.to('cuda:3')\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Get embeddings or representations from model\n",
    "        rgb_emb  = model(rgb_data)\n",
    "        ir_emb  = ir_model(ir_data)\n",
    "\n",
    "        # Compute the contrastive loss\n",
    "        loss = info_nce_loss(rgb_emb, ir_emb, temperature)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        #break\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    ir_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for batch_data, _ in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "            rgb_data = batch_data['rgb'].cuda() #.to('cuda:3')\n",
    "            ir_data = batch_data['ir'].cuda() #.to('cuda:3')\n",
    "            rgb_emb  = model(rgb_data)\n",
    "            ir_emb  = ir_model(ir_data)\n",
    "            loss = info_nce_loss(rgb_emb, ir_emb, temperature)\n",
    "            val_loss += loss.item()\n",
    "            #break\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save the best model (optional)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_ir_encoder.pth')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "\n",
    "# Load the base CLIPTextConfig\n",
    "clip_text_config = CLIPTextConfig.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "class CustomCLIPTextModel(CLIPTextModel):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__(config)\n",
    "        # No additional text config passed here as it's not provided\n",
    "        self.text_model = CLIPTextTransformer(config)\n",
    "        \n",
    "        self.text_projection = nn.Linear(in_features=512, out_features=512, bias=False)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        # Call the original forward method to get the model outputs\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        \n",
    "        # Apply the text_projection layer to the pooled_output (assuming you want to project the pooled output)\n",
    "        projected_output = self.text_projection(outputs.pooler_output)\n",
    "        \n",
    "        if not return_dict:\n",
    "            # If not returning a dict, convert the BaseModelOutputWithPooling to a tuple,\n",
    "            # append the projected_output to the tuple, and return\n",
    "            outputs_tuple = (\n",
    "                outputs.last_hidden_state,\n",
    "                projected_output,\n",
    "                outputs.hidden_states,\n",
    "                outputs.attentions\n",
    "            )\n",
    "            return outputs_tuple\n",
    "        \n",
    "        # Otherwise, create a new BaseModelOutputWithPooling containing the projected_output and return\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            pooler_output=projected_output,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "# Instantiate the text model with the loaded configuration\n",
    "text_model = CustomCLIPTextModel(clip_text_config)\n",
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to hold the relevant weights\n",
    "state_dict = {}\n",
    "\n",
    "# Copy weights from the vidclip_model_weights to state_dict\n",
    "for name, param in vidclip_model_weights.items():\n",
    "    if \"text_model\" in name:\n",
    "        new_name = name.replace(\"clipmodel.\", \"\")  # remove the prefix\n",
    "        state_dict[new_name] = param\n",
    "    if \"text_projection\" in name:\n",
    "        new_name = name.replace(\"clipmodel.\", \"\")  # remove the prefix\n",
    "        state_dict[new_name] = param\n",
    "\n",
    "# Load the state_dict into clip_vision_model\n",
    "text_model.load_state_dict(state_dict)\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "\n",
    "text_model = text_model.to('cuda:3')\n",
    "clip_model_name = \"openai/clip-vit-base-patch16\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "\n",
    "text_descriptions = [\n",
    "    \"drink water\",\n",
    "    \"eat meal\",\n",
    "    \"brush teeth\",\n",
    "    \"brush hair\",\n",
    "    \"drop\",\n",
    "    \"pick up\",\n",
    "    \"throw\",\n",
    "    \"sit down\",\n",
    "    \"stand up\",\n",
    "    \"clapping\",\n",
    "    \"reading\",\n",
    "    \"writing\",\n",
    "    \"tear up paper\",\n",
    "    \"put on jacket\",\n",
    "    \"take off jacket\",\n",
    "    \"put on a shoe\",\n",
    "    \"take off a shoe\",\n",
    "    \"put on glasses\",\n",
    "    \"take off glasses\",\n",
    "    \"put on a hat/cap\",\n",
    "    \"take off a hat/cap\",\n",
    "    \"cheer up\",\n",
    "    \"hand waving\",\n",
    "    \"kicking something\",\n",
    "    \"reach into pocket\",\n",
    "    \"hopping\",\n",
    "    \"jump up\",\n",
    "    \"phone call\",\n",
    "    \"play with phone/tablet\",\n",
    "    \"typing on a keyboard\",\n",
    "    \"point to something\",\n",
    "    \"taking a selfie\",\n",
    "    \"check time (from watch)\",\n",
    "    \"rub two hands\",\n",
    "    \"nod head/bow\",\n",
    "    \"shake head\",\n",
    "    \"wipe face\",\n",
    "    \"salute\",\n",
    "    \"put palms together\",\n",
    "    \"cross hands in front\",\n",
    "    \"sneeze/cough\",\n",
    "    \"staggering\",\n",
    "    \"falling down\",\n",
    "    \"headache\",\n",
    "    \"chest pain\",\n",
    "    \"back pain\",\n",
    "    \"neck pain\",\n",
    "    \"nausea/vomiting\",\n",
    "    \"fan self\",\n",
    "    \"punch/slapp\",\n",
    "    \"kicking\",\n",
    "    \"pushing\",\n",
    "    \"pat on back\",\n",
    "    \"point finger\",\n",
    "    \"hugging\",\n",
    "    \"giving object\",\n",
    "    \"touch pocket\",\n",
    "    \"shaking hands\",\n",
    "    \"walking towards\",\n",
    "    \"walking apart\"\n",
    "]\n",
    "\n",
    "# Tokenize the text descriptions\n",
    "text_inputs = tokenizer(text_descriptions, return_tensors=\"pt\", padding=True, truncation=True).to('cuda:3')\n",
    "\n",
    "# Create dummy pixel values\n",
    "batch_size = text_inputs['input_ids'].shape[0]\n",
    "\n",
    "\n",
    "# Obtain text embeddings using the text model of CLIP\n",
    "with torch.no_grad():\n",
    "    text_outputs = text_model(input_ids=text_inputs['input_ids'])\n",
    "    text_embeddings = text_outputs\n",
    "\n",
    "print(text_embeddings[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionViPEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (position_embedding): Embedding(197, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                           | 0/711 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23416, 42806, 5234, 31987, 53795, 39887, 51540, 10851]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                 | 1/711 [00:17<3:26:14, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([53,  8,  8,  8,  8,  8,  2,  8], device='cuda:3')\n",
      "tensor([16, 26, 14,  7, 35, 47,  0, 51], device='cuda:3')\n",
      "[29637, 1159, 7476, 16431, 20799, 52087, 52150, 41367]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|▏                                                | 2/711 [00:17<1:26:34,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8, 20, 27, 53, 53,  8,  8,  8], device='cuda:3')\n",
      "tensor([57, 19, 36, 51, 39,  7, 10, 27], device='cuda:3')\n",
      "[6219, 10279, 8656, 23902, 9015, 16629, 1397, 28602]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|▏                                                  | 3/711 [00:17<48:21,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31,  8,  8, 39,  8, 53,  8,  8], device='cuda:3')\n",
      "tensor([39, 19, 16, 22, 15,  9, 17, 42], device='cuda:3')\n",
      "[47820, 44664, 25441, 26051, 32787, 17716, 40133, 45648]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▎                                                  | 4/711 [00:18<30:24,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8, 29, 53, 55, 31, 53, 53,  8], device='cuda:3')\n",
      "tensor([ 0, 24,  1, 11, 27, 16, 53, 48], device='cuda:3')\n",
      "[37058, 41083, 2651, 11199, 12412, 43992, 14099, 23274]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▎                                                  | 5/711 [00:18<20:30,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29,  8,  8, 53, 53, 29,  8, 54], device='cuda:3')\n",
      "tensor([38, 43, 11, 39, 52, 12, 59, 54], device='cuda:3')\n",
      "[49571, 3526, 40385, 13576, 39547, 1343, 7133, 8908]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▍                                                  | 6/711 [00:18<14:32,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29,  8,  8,  8,  8,  8, 53, 31], device='cuda:3')\n",
      "tensor([11, 46,  5, 16,  7, 23, 53, 28], device='cuda:3')\n",
      "[41591, 32699, 6608, 38317, 20560, 16444, 49489, 54842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▌                                                  | 7/711 [00:18<10:45,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29, 29, 29, 31,  8, 55, 53, 53], device='cuda:3')\n",
      "tensor([11, 59,  8, 37, 40,  4, 49,  2], device='cuda:3')\n",
      "[2676, 5147, 14576, 12730, 25412, 37438, 31811, 55466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▌                                                  | 8/711 [00:19<08:16,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18,  8, 53,  8, 53, 29, 29,  8], device='cuda:3')\n",
      "tensor([36, 47, 56, 10, 32, 58, 11, 26], device='cuda:3')\n",
      "[6440, 12557, 29337, 12734, 36598, 25210, 3755, 31339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▋                                                  | 9/711 [00:19<06:37,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([53, 31, 57, 14, 29, 53,  8, 19], device='cuda:3')\n",
      "tensor([20, 17, 57, 14, 58, 10, 35, 19], device='cuda:3')\n",
      "[2094, 11298, 18856, 53395, 5556, 32682, 9351, 3620]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|▋                                                 | 10/711 [00:19<05:29,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([54, 53, 29,  8, 53,  8, 53, 20], device='cuda:3')\n",
      "tensor([54, 18, 16, 55, 36, 42, 51, 20], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def batch_cosine_similarity(x1, x2):\n",
    "    # x1 has shape (batch_size, embed_dim)\n",
    "    # x2 has shape (num_text_descriptions, embed_dim)\n",
    "    dot = x1 @ x2.T\n",
    "    norm1 = torch.norm(x1, p=2, dim=1).unsqueeze(1)\n",
    "    norm2 = torch.norm(x2, p=2, dim=1).unsqueeze(0)\n",
    "    return dot / (norm1 * norm2)\n",
    "print(model)\n",
    "model = model.to('cuda:3')\n",
    "correct_rgb_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels, idx in tqdm(test_loader, desc=\"Evaluating\", ncols=100):\n",
    "        # Move data to the appropriate device\n",
    "        rgb_data = batch_data['rgb'].to('cuda:3')\n",
    "        batch_labels = batch_labels.to('cuda:3')\n",
    "        print(idx)\n",
    "\n",
    "        model.eval()\n",
    "        # Extract embeddings from the model\n",
    "        rgb_emb = model(rgb_data)\n",
    "        \n",
    "        # Compute cosine similarities for both RGB and IR embeddings\n",
    "        similarities_rgb = batch_cosine_similarity(rgb_emb, text_embeddings[1])\n",
    "        \n",
    "        \n",
    "        # Get predicted classes\n",
    "        predicted_class_rgb = torch.argmax(similarities_rgb, dim=1)\n",
    "        print(predicted_class_rgb)\n",
    "        print(batch_labels)\n",
    "        # Update correct predictions count\n",
    "        correct_rgb_predictions += (predicted_class_rgb == batch_labels).sum().item()\n",
    "        \n",
    "        \n",
    "        # Update total samples count\n",
    "        total_samples += batch_labels.size(0)\n",
    "\n",
    "# Compute accuracies\n",
    "accuracy_rgb = correct_rgb_predictions / total_samples\n",
    "\n",
    "\n",
    "print(f\"RGB Accuracy: {accuracy_rgb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_descriptions60_120 = [\n",
    "    \"put on headphone\",\n",
    "    \"take off headphone\",\n",
    "    \"shoot at the basket\",\n",
    "    \"bounce ball\",\n",
    "    \"tennis bat swing\",\n",
    "    \"juggling table tennis balls\",\n",
    "    \"hush (quite)\",\n",
    "    \"flick hair\",\n",
    "    \"thumb up\",\n",
    "    \"thumb down\",\n",
    "    \"make ok sign\",\n",
    "    \"make victory sign\",\n",
    "    \"staple book\",\n",
    "    \"counting money\",\n",
    "    \"cutting nails\",\n",
    "    \"cutting paper (using scissors)\",\n",
    "    \"snapping fingers\",\n",
    "    \"open bottle\",\n",
    "    \"sniff (smell)\",\n",
    "    \"squat down\",\n",
    "    \"toss a coin\",\n",
    "    \"fold paper\",\n",
    "    \"ball up paper\",\n",
    "    \"play magic cube\",\n",
    "    \"apply cream on face\",\n",
    "    \"apply cream on hand back\",\n",
    "    \"put on bag\",\n",
    "    \"take off bag\",\n",
    "    \"put something into a bag\",\n",
    "    \"take something out of a bag\",\n",
    "    \"open a box\",\n",
    "    \"move heavy objects\",\n",
    "    \"shake fist\",\n",
    "    \"throw up cap/hat\",\n",
    "    \"hands up (both hands)\",\n",
    "    \"cross arms\",\n",
    "    \"arm circles\",\n",
    "    \"arm swings\",\n",
    "    \"running on the spot\",\n",
    "    \"butt kicks (kick backward)\",\n",
    "    \"cross toe touch\",\n",
    "    \"side kick\",\n",
    "    \"yawn\",\n",
    "    \"stretch oneself\",\n",
    "    \"blow nose\",\n",
    "    \"hit other person with something\",\n",
    "    \"wield knife towards other person\",\n",
    "    \"knock over other person (hit with body)\",\n",
    "    \"grab other person’s stuff\",\n",
    "    \"shoot at other person with a gun\",\n",
    "    \"step on foot\",\n",
    "    \"high-five\",\n",
    "    \"cheers and drink\",\n",
    "    \"carry something with other person\",\n",
    "    \"take a photo of other person\",\n",
    "    \"follow other person\",\n",
    "    \"whisper in other person’s ear\",\n",
    "    \"exchange things with other person\",\n",
    "    \"support somebody with hand\",\n",
    "    \"finger-guessing game (playing rock-paper-scissors)\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "text_descriptions = [\n",
    "    \"drink water\",\n",
    "    \"eat meal\",\n",
    "    \"brush teeth\",\n",
    "    \"brush hair\",\n",
    "    \"drop\",\n",
    "    \"pick up\",\n",
    "    \"throw\",\n",
    "    \"sit down\",\n",
    "    \"stand up\",\n",
    "    \"clapping\",\n",
    "    \"reading\",\n",
    "    \"writing\",\n",
    "    \"tear up paper\",\n",
    "    \"put on jacket\",\n",
    "    \"take off jacket\",\n",
    "    \"put on a shoe\",\n",
    "    \"take off a shoe\",\n",
    "    \"put on glasses\",\n",
    "    \"take off glasses\",\n",
    "    \"put on a hat/cap\",\n",
    "    \"take off a hat/cap\",\n",
    "    \"cheer up\",\n",
    "    \"hand waving\",\n",
    "    \"kicking something\",\n",
    "    \"reach into pocket\",\n",
    "    \"hopping\",\n",
    "    \"jump up\",\n",
    "    \"phone call\",\n",
    "    \"play with phone/tablet\",\n",
    "    \"typing on a keyboard\",\n",
    "    \"point to something\",\n",
    "    \"taking a selfie\",\n",
    "    \"check time (from watch)\",\n",
    "    \"rub two hands\",\n",
    "    \"nod head/bow\",\n",
    "    \"shake head\",\n",
    "    \"wipe face\",\n",
    "    \"salute\",\n",
    "    \"put palms together\",\n",
    "    \"cross hands in front\",\n",
    "    \"sneeze/cough\",\n",
    "    \"staggering\",\n",
    "    \"falling down\",\n",
    "    \"headache\",\n",
    "    \"chest pain\",\n",
    "    \"back pain\",\n",
    "    \"neck pain\",\n",
    "    \"nausea/vomiting\",\n",
    "    \"fan self\",\n",
    "    \"punch/slapp\",\n",
    "    \"kicking\",\n",
    "    \"pushing\",\n",
    "    \"pat on back\",\n",
    "    \"point finger\",\n",
    "    \"hugging\",\n",
    "    \"giving object\",\n",
    "    \"touch pocket\",\n",
    "    \"shaking hands\",\n",
    "    \"walking towards\",\n",
    "    \"walking apart\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_descriptions120 = [\n",
    "    \"A person putting on headphones in a quiet study room\",\n",
    "    \"An individual taking off headphones in a home office with a computer.\",\n",
    "    \"Someone shooting a basketball towards a hoop in an outdoor court during sunset.\",\n",
    "    \"A person bouncing a ball on a paved driveway with a basketball hoop in the background.\",\n",
    "    \"An individual swinging a tennis racket at a yellow ball on a sunny tennis court.\",\n",
    "    \"Someone juggling table tennis balls in a bright game room with a ping pong table.\",\n",
    "    \"A person gesturing 'hush' with a finger on lips in a library filled with bookshelves.\",\n",
    "    \"An individual flicking their long hair back in a mirror reflection in a dance studio.\",\n",
    "    \"A person giving a thumbs up in a bright classroom with students and a chalkboard.\",\n",
    "    \"Someone giving a thumbs down in a meeting room with a large monitor displaying data.\",\n",
    "    \"An individual making an OK sign with their hand in a cozy cafe with coffee cups on tables.\",\n",
    "    \"A person making a victory sign with their fingers in front of a scenic viewpoint overlooking mountains.\",\n",
    "    \"Someone stapling pages of a book in a crafting room with art supplies on shelves.\",\n",
    "    \"An individual counting money on a wooden table in a small business office.\",\n",
    "    \"A person cutting nails sitting on a porch with a garden view.\",\n",
    "    \"Someone cutting paper with scissors on a cluttered craft table in an art studio.\",\n",
    "    \"An individual snapping fingers to music in a brightly lit kitchen while cooking.\",\n",
    "    \"A person opening a bottle in a picnic setting with a basket and a blanket.\",\n",
    "    \"Someone sniffing a perfume bottle in a boutique with shelves of beauty products.\",\n",
    "    \"An individual squatting down to tie shoelaces in a gym with workout equipment.\",\n",
    "    \"A person tossing a coin into a fountain outdoors with trees in the background.\",\n",
    "    \"Someone folding paper into an airplane in a classroom with kids' drawings on the walls.\",\n",
    "    \"An individual balling up paper in frustration in an office with a computer and documents.\",\n",
    "    \"A person playing with a magic cube in a cozy living room on a soft rug.\",\n",
    "    \"Someone applying cream on their face in a bright bathroom with a large mirror.\",\n",
    "    \"An individual applying cream on the back of their hand in a beauty salon with products on display.\",\n",
    "    \"A person putting on a backpack in a hostel dormitory with bunk beds and lockers.\",\n",
    "    \"Someone taking off a backpack at the entrance of a hiking trail with woods in the background.\",\n",
    "    \"An individual putting something into a bag on a kitchen counter with groceries around.\",\n",
    "    \"A person taking something out of a bag in a classroom with desks and a projector.\",\n",
    "    \"Someone opening a box in a living room on Christmas morning with decorations.\",\n",
    "    \"An individual moving heavy objects in a garage filled with tools and storage boxes.\",\n",
    "    \"A person shaking their fist in excitement at a sports event with a crowd cheering.\",\n",
    "    \"Someone throwing up a cap in celebration during a graduation ceremony in an open field.\",\n",
    "    \"An individual with hands up in surrender during a playful water fight in a backyard.\",\n",
    "    \"A person crossing their arms while waiting in a coffee shop with a line of customers.\",\n",
    "    \"Someone doing arm circles as part of a warm-up in a fitness class at a gym.\",\n",
    "    \"An individual performing arm swings in a park with autumn leaves on the ground.\",\n",
    "    \"A person running on the spot in a home gym with a treadmill and weights.\",\n",
    "    \"Someone doing butt kicks during a track workout on a sunny day at an athletic field.\",\n",
    "    \"An individual performing a cross toe touch in a yoga studio with mats and calming decor.\",\n",
    "    \"A person executing a side kick in a martial arts dojo with mirrors and training pads.\",\n",
    "    \"Someone yawning widely in a cozy bedroom early in the morning with the sun rising.\",\n",
    "    \"An individual stretching themselves in an office during a break with a coffee cup on the desk.\",\n",
    "    \"A person blowing their nose with a tissue in a bright, airy living room with a large window.\",\n",
    "    \"Someone hitting another person with a foam bat in a playful outdoor party setting.\",\n",
    "    \"An individual wielding a knife towards another person in a dramatic theater rehearsal scene.\",\n",
    "    \"A person knocking over another person during a friendly beach volleyball game.\",\n",
    "    \"Someone grabbing another person’s hat in a playful manner at a sunny park.\",\n",
    "    \"An individual shooting at another person with a water gun during a summer backyard party.\",\n",
    "    \"A person stepping on someone’s foot accidentally in a crowded subway car.\",\n",
    "    \"Someone high-fiving in a sports team huddle on a field with goals in the background.\",\n",
    "    \"An individual cheering and drinking with friends at a rooftop bar with city lights.\",\n",
    "    \"Two people carrying a couch together into a new apartment with boxes around.\",\n",
    "    \"A person taking a photo of another person in front of a famous landmark during a trip.\",\n",
    "    \"Someone following another person in a vast open room.\",\n",
    "    \"An individual whispering in another person’s ear during a secret exchange in a library.\",\n",
    "    \"Two people exchanging things in an industrial building.\",\n",
    "    \"Someone supporting somebody with a hand during a difficult hiking trail with scenic views.\",\n",
    "    \"Two individuals playing rock-paper-scissors in a schoolyard with children watching.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "text_descriptions = [\n",
    "    \"A person drinking water from a clear glass in a kitchen.\",\n",
    "    \"An individual eating a meal at a dining table, using a fork and knife.\",\n",
    "    \"A person brushing teeth with a toothbrush in a bathroom mirror.\",\n",
    "    \"Someone brushing long hair with a hairbrush in a bedroom.\",\n",
    "    \"A person dropping a red ball onto a wooden floor in a living room.\",\n",
    "    \"An individual picking up a blue book from the floor in a study room.\",\n",
    "    \"A person throwing a white paper airplane in an office setting.\",\n",
    "    \"Someone sitting down on a green armchair in a cozy room.\",\n",
    "    \"An individual standing up from a metal chair in a cafeteria.\",\n",
    "    \"A person clapping hands in an auditorium with a stage.\",\n",
    "    \"Someone reading a hardcover book in a library with bookshelves.\",\n",
    "    \"An individual writing in a notebook at a desk with a lamp.\",\n",
    "    \"A person tearing up a sheet of paper over a trash bin in a workspace.\",\n",
    "    \"Someone putting on a black jacket in a hallway with coat hangers.\",\n",
    "    \"An individual taking off a red jacket in a changing room.\",\n",
    "    \"A person putting on a white sneaker in a gym locker room.\",\n",
    "    \"Someone taking off a brown shoe in an entryway with a shoe rack.\",\n",
    "    \"An individual putting on eyeglasses in an office with a computer.\",\n",
    "    \"A person taking off sunglasses in a sunlit atrium.\",\n",
    "    \"Someone putting on a baseball cap in a sports store.\",\n",
    "    \"An individual taking off a wool hat in a coat room.\",\n",
    "    \"A person cheering up, smiling and laughing in a living room with a sofa.\",\n",
    "    \"Someone waving hand in a greeting at a hotel lobby.\",\n",
    "    \"An individual kicking a small football in an indoor play area.\",\n",
    "    \"A person reaching into a pocket of jeans in a bedroom.\",\n",
    "    \"Someone hopping on one foot in a fitness studio.\",\n",
    "    \"An individual jumping up with arms raised in a dance studio.\",\n",
    "    \"A person making a phone call on a smartphone in a home office.\",\n",
    "    \"Someone playing with a tablet on a couch in a family room.\",\n",
    "    \"An individual typing on a keyboard at a computer desk in a study.\",\n",
    "    \"A person pointing to a painting on a wall in an art gallery.\",\n",
    "    \"Someone taking a selfie with a phone in a mirror in a dressing room.\",\n",
    "    \"An individual checking time on a wristwatch in a conference room.\",\n",
    "    \"A person rubbing two hands together in a kitchen.\",\n",
    "    \"Someone nodding head in agreement in a meeting room with a whiteboard.\",\n",
    "    \"An individual shaking head in disapproval in a classroom.\",\n",
    "    \"A person wiping face with a handkerchief in a bathroom.\",\n",
    "    \"Someone saluting in a uniform in a military office.\",\n",
    "    \"An individual putting palms together in a gesture of prayer in a chapel.\",\n",
    "    \"A person crossing arms in front in a casual home setting.\",\n",
    "    \"Someone sneezing into a tissue in a doctor's waiting room.\",\n",
    "    \"An individual staggering in a hallway as if dizzy.\",\n",
    "    \"A person falling down onto a carpet in a living room.\",\n",
    "    \"Someone holding head in pain, indicating a headache, in an office.\",\n",
    "    \"An individual clutching chest in pain in a home living area.\",\n",
    "    \"A person holding lower back in pain in a furniture store.\",\n",
    "    \"Someone holding neck in pain in a home study.\",\n",
    "    \"An individual feeling nauseous, about to vomit, in a bathroom.\",\n",
    "    \"A person fanning self with a magazine in a warm room.\",\n",
    "    \"Someone punching the air in a boxing gym.\",\n",
    "    \"An individual kicking a pillow in a bedroom.\",\n",
    "    \"A person pushing a chair in a dining room.\",\n",
    "    \"Someone patting a friend on the back in a coffee shop.\",\n",
    "    \"An individual pointing a finger at a computer screen in an office.\",\n",
    "    \"A person hugging a friend in a living room.\",\n",
    "    \"Someone giving a pen to another person in an office.\",\n",
    "    \"An individual touching the pocket of their jeans in a bedroom.\",\n",
    "    \"Two people shaking hands in a business meeting room.\",\n",
    "    \"A person walking towards a window in a bright room.\",\n",
    "    \"Two individuals walking apart in a hallway of an office building.\"\n",
    "]\n",
    "\n",
    "print(len(text_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "text_descriptions120 = [\n",
    "    \"A person drinking water from a clear glass in a kitchen.\",\n",
    "    \"An individual eating a meal at a dining table, using a fork and knife.\",\n",
    "    \"A person brushing teeth with a toothbrush in a bathroom mirror.\",\n",
    "    \"Someone brushing long hair with a hairbrush in a bedroom.\",\n",
    "    \"A person dropping a red ball onto a wooden floor in a living room.\",\n",
    "    \"An individual picking up a blue book from the floor in a study room.\",\n",
    "    \"A person throwing a white paper airplane in an office setting.\",\n",
    "    \"Someone sitting down on a green armchair in a cozy room.\",\n",
    "    \"An individual standing up from a metal chair in a cafeteria.\",\n",
    "    \"A person clapping hands in an auditorium with a stage.\",\n",
    "    \"Someone reading a hardcover book in a library with bookshelves.\",\n",
    "    \"An individual writing in a notebook at a desk with a lamp.\",\n",
    "    \"A person tearing up a sheet of paper over a trash bin in a workspace.\",\n",
    "    \"Someone putting on a black jacket in a hallway with coat hangers.\",\n",
    "    \"An individual taking off a red jacket in a changing room.\",\n",
    "    \"A person putting on a white sneaker in a gym locker room.\",\n",
    "    \"Someone taking off a brown shoe in an entryway with a shoe rack.\",\n",
    "    \"An individual putting on eyeglasses in an office with a computer.\",\n",
    "    \"A person taking off sunglasses in a sunlit atrium.\",\n",
    "    \"Someone putting on a baseball cap in a sports store.\",\n",
    "    \"An individual taking off a wool hat in a coat room.\",\n",
    "    \"A person cheering up, smiling and laughing in a living room with a sofa.\",\n",
    "    \"Someone waving hand in a greeting at a hotel lobby.\",\n",
    "    \"An individual kicking a small football in an indoor play area.\",\n",
    "    \"A person reaching into a pocket of jeans in a bedroom.\",\n",
    "    \"Someone hopping on one foot in a fitness studio.\",\n",
    "    \"An individual jumping up with arms raised in a dance studio.\",\n",
    "    \"A person making a phone call on a smartphone in a home office.\",\n",
    "    \"Someone playing with a tablet on a couch in a family room.\",\n",
    "    \"An individual typing on a keyboard at a computer desk in a study.\",\n",
    "    \"A person pointing to a painting on a wall in an art gallery.\",\n",
    "    \"Someone taking a selfie with a phone in a mirror in a dressing room.\",\n",
    "    \"An individual checking time on a wristwatch in a conference room.\",\n",
    "    \"A person rubbing two hands together in a kitchen.\",\n",
    "    \"Someone nodding head in agreement in a meeting room with a whiteboard.\",\n",
    "    \"An individual shaking head in disapproval in a classroom.\",\n",
    "    \"A person wiping face with a handkerchief in a bathroom.\",\n",
    "    \"Someone saluting in a uniform in a military office.\",\n",
    "    \"An individual putting palms together in a gesture of prayer in a chapel.\",\n",
    "    \"A person crossing arms in front in a casual home setting.\",\n",
    "    \"Someone sneezing into a tissue in a doctor's waiting room.\",\n",
    "    \"An individual staggering in a hallway as if dizzy.\",\n",
    "    \"A person falling down onto a carpet in a living room.\",\n",
    "    \"Someone holding head in pain, indicating a headache, in an office.\",\n",
    "    \"An individual clutching chest in pain in a home living area.\",\n",
    "    \"A person holding lower back in pain in a furniture store.\",\n",
    "    \"Someone holding neck in pain in a home study.\",\n",
    "    \"An individual feeling nauseous, about to vomit, in a bathroom.\",\n",
    "    \"A person fanning self with a magazine in a warm room.\",\n",
    "    \"Someone punching the air in a boxing gym.\",\n",
    "    \"An individual kicking a pillow in a bedroom.\",\n",
    "    \"A person pushing a chair in a dining room.\",\n",
    "    \"Someone patting a friend on the back in a coffee shop.\",\n",
    "    \"An individual pointing a finger at a computer screen in an office.\",\n",
    "    \"A person hugging a friend in a living room.\",\n",
    "    \"Someone giving a pen to another person in an office.\",\n",
    "    \"An individual touching the pocket of their jeans in a bedroom.\",\n",
    "    \"Two people shaking hands in a business meeting room.\",\n",
    "    \"A person walking towards a window in a bright room.\",\n",
    "    \"Two individuals walking apart in a hallway of an office building.\",\n",
    "    \"A person putting on headphones in a quiet study room\",\n",
    "    \"An individual taking off headphones in a home office with a computer.\",\n",
    "    \"Someone shooting a basketball towards a hoop in an outdoor court during sunset.\",\n",
    "    \"A person bouncing a ball on a paved driveway with a basketball hoop in the background.\",\n",
    "    \"An individual swinging a tennis racket at a yellow ball on a sunny tennis court.\",\n",
    "    \"Someone juggling table tennis balls in a bright game room with a ping pong table.\",\n",
    "    \"A person gesturing 'hush' with a finger on lips in a library filled with bookshelves.\",\n",
    "    \"An individual flicking their long hair back in a mirror reflection in a dance studio.\",\n",
    "    \"A person giving a thumbs up in a bright classroom with students and a chalkboard.\",\n",
    "    \"Someone giving a thumbs down in a meeting room with a large monitor displaying data.\",\n",
    "    \"An individual making an OK sign with their hand in a cozy cafe with coffee cups on tables.\",\n",
    "    \"A person making a victory sign with their fingers in front of a scenic viewpoint overlooking mountains.\",\n",
    "    \"Someone stapling pages of a book in a crafting room with art supplies on shelves.\",\n",
    "    \"An individual counting money on a wooden table in a small business office.\",\n",
    "    \"A person cutting nails sitting on a porch with a garden view.\",\n",
    "    \"Someone cutting paper with scissors on a cluttered craft table in an art studio.\",\n",
    "    \"An individual snapping fingers to music in a brightly lit kitchen while cooking.\",\n",
    "    \"A person opening a bottle in a picnic setting with a basket and a blanket.\",\n",
    "    \"Someone sniffing a perfume bottle in a boutique with shelves of beauty products.\",\n",
    "    \"An individual squatting down to tie shoelaces in a gym with workout equipment.\",\n",
    "    \"A person tossing a coin into a fountain outdoors with trees in the background.\",\n",
    "    \"Someone folding paper into an airplane in a classroom with kids' drawings on the walls.\",\n",
    "    \"An individual balling up paper in frustration in an office with a computer and documents.\",\n",
    "    \"A person playing with a magic cube in a cozy living room on a soft rug.\",\n",
    "    \"Someone applying cream on their face in a bright bathroom with a large mirror.\",\n",
    "    \"An individual applying cream on the back of their hand in a beauty salon with products on display.\",\n",
    "    \"A person putting on a backpack in a hostel dormitory with bunk beds and lockers.\",\n",
    "    \"Someone taking off a backpack at the entrance of a hiking trail with woods in the background.\",\n",
    "    \"An individual putting something into a bag on a kitchen counter with groceries around.\",\n",
    "    \"A person taking something out of a bag in a classroom with desks and a projector.\",\n",
    "    \"Someone opening a box in a living room on Christmas morning with decorations.\",\n",
    "    \"An individual moving heavy objects in a garage filled with tools and storage boxes.\",\n",
    "    \"A person shaking their fist in excitement at a sports event with a crowd cheering.\",\n",
    "    \"Someone throwing up a cap in celebration during a graduation ceremony in an open field.\",\n",
    "    \"An individual with hands up in surrender during a playful water fight in a backyard.\",\n",
    "    \"A person crossing their arms while waiting in a coffee shop with a line of customers.\",\n",
    "    \"Someone doing arm circles as part of a warm-up in a fitness class at a gym.\",\n",
    "    \"An individual performing arm swings in a park with autumn leaves on the ground.\",\n",
    "    \"A person running on the spot in a home gym with a treadmill and weights.\",\n",
    "    \"Someone doing butt kicks during a track workout on a sunny day at an athletic field.\",\n",
    "    \"An individual performing a cross toe touch in a yoga studio with mats and calming decor.\",\n",
    "    \"A person executing a side kick in a martial arts dojo with mirrors and training pads.\",\n",
    "    \"Someone yawning widely in a cozy bedroom early in the morning with the sun rising.\",\n",
    "    \"An individual stretching themselves in an office during a break with a coffee cup on the desk.\",\n",
    "    \"A person blowing their nose with a tissue in a bright, airy living room with a large window.\",\n",
    "    \"Someone hitting another person with a foam bat in a playful outdoor party setting.\",\n",
    "    \"An individual wielding a knife towards another person in a dramatic theater rehearsal scene.\",\n",
    "    \"A person knocking over another person during a friendly beach volleyball game.\",\n",
    "    \"Someone grabbing another person’s hat in a playful manner at a sunny park.\",\n",
    "    \"An individual shooting at another person with a water gun during a summer backyard party.\",\n",
    "    \"A person stepping on someone’s foot accidentally in a crowded subway car.\",\n",
    "    \"Someone high-fiving in a sports team huddle on a field with goals in the background.\",\n",
    "    \"An individual cheering and drinking with friends at a rooftop bar with city lights.\",\n",
    "    \"Two people carrying a couch together into a new apartment with boxes around.\",\n",
    "    \"A person taking a photo of another person in front of a famous landmark during a trip.\",\n",
    "    \"Someone following another person in a vast open room.\",\n",
    "    \"An individual whispering in another person’s ear during a secret exchange in a library.\",\n",
    "    \"Two people exchanging things in an industrial building.\",\n",
    "    \"Someone supporting somebody with a hand during a difficult hiking trail with scenic views.\",\n",
    "    \"Two individuals playing rock-paper-scissors in a schoolyard with children watching.\"\n",
    "]\n",
    "\n",
    "print(len(text_descriptions120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'drink water', 2: 'eat meal', 3: 'brush teeth', 4: 'brush hair', 5: 'drop', 6: 'pick up', 7: 'throw', 8: 'sit down', 9: 'stand up', 10: 'clapping', 11: 'reading', 12: 'writing', 13: 'tear up paper', 14: 'put on jacket', 15: 'take off jacket', 16: 'put on a shoe', 17: 'take off a shoe', 18: 'put on glasses', 19: 'take off glasses', 20: 'put on a hat/cap', 21: 'take off a hat/cap', 22: 'cheer up', 23: 'hand waving', 24: 'kicking something', 25: 'reach into pocket', 26: 'hopping', 27: 'jump up', 28: 'phone call', 29: 'play with phone/tablet', 30: 'typing on a keyboard', 31: 'point to something', 32: 'taking a selfie', 33: 'check time (from watch)', 34: 'rub two hands', 35: 'nod head/bow', 36: 'shake head', 37: 'wipe face', 38: 'salute', 39: 'put palms together', 40: 'cross hands in front', 41: 'sneeze/cough', 42: 'staggering', 43: 'falling down', 44: 'headache', 45: 'chest pain', 46: 'back pain', 47: 'neck pain', 48: 'nausea/vomiting', 49: 'fan self', 50: 'punch/slapp', 51: 'kicking', 52: 'pushing', 53: 'pat on back', 54: 'point finger', 55: 'hugging', 56: 'giving object', 57: 'touch pocket', 58: 'shaking hands', 59: 'walking towards', 60: 'walking apart'}\n"
     ]
    }
   ],
   "source": [
    "class_index_to_text_description = {index: description for index, description in enumerate(text_descriptions, 1)}\n",
    "print(class_index_to_text_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'A person drinking water from a clear glass in a kitchen.', 2: 'An individual eating a meal at a dining table, using a fork and knife.', 3: 'A person brushing teeth with a toothbrush in a bathroom mirror.', 4: 'Someone brushing long hair with a hairbrush in a bedroom.', 5: 'A person dropping a red ball onto a wooden floor in a living room.', 6: 'An individual picking up a blue book from the floor in a study room.', 7: 'A person throwing a white paper airplane in an office setting.', 8: 'Someone sitting down on a green armchair in a cozy room.', 9: 'An individual standing up from a metal chair in a cafeteria.', 10: 'A person clapping hands in an auditorium with a stage.', 11: 'Someone reading a hardcover book in a library with bookshelves.', 12: 'An individual writing in a notebook at a desk with a lamp.', 13: 'A person tearing up a sheet of paper over a trash bin in a workspace.', 14: 'Someone putting on a black jacket in a hallway with coat hangers.', 15: 'An individual taking off a red jacket in a changing room.', 16: 'A person putting on a white sneaker in a gym locker room.', 17: 'Someone taking off a brown shoe in an entryway with a shoe rack.', 18: 'An individual putting on eyeglasses in an office with a computer.', 19: 'A person taking off sunglasses in a sunlit atrium.', 20: 'Someone putting on a baseball cap in a sports store.', 21: 'An individual taking off a wool hat in a coat room.', 22: 'A person cheering up, smiling and laughing in a living room with a sofa.', 23: 'Someone waving hand in a greeting at a hotel lobby.', 24: 'An individual kicking a small football in an indoor play area.', 25: 'A person reaching into a pocket of jeans in a bedroom.', 26: 'Someone hopping on one foot in a fitness studio.', 27: 'An individual jumping up with arms raised in a dance studio.', 28: 'A person making a phone call on a smartphone in a home office.', 29: 'Someone playing with a tablet on a couch in a family room.', 30: 'An individual typing on a keyboard at a computer desk in a study.', 31: 'A person pointing to a painting on a wall in an art gallery.', 32: 'Someone taking a selfie with a phone in a mirror in a dressing room.', 33: 'An individual checking time on a wristwatch in a conference room.', 34: 'A person rubbing two hands together in a kitchen.', 35: 'Someone nodding head in agreement in a meeting room with a whiteboard.', 36: 'An individual shaking head in disapproval in a classroom.', 37: 'A person wiping face with a handkerchief in a bathroom.', 38: 'Someone saluting in a uniform in a military office.', 39: 'An individual putting palms together in a gesture of prayer in a chapel.', 40: 'A person crossing arms in front in a casual home setting.', 41: \"Someone sneezing into a tissue in a doctor's waiting room.\", 42: 'An individual staggering in a hallway as if dizzy.', 43: 'A person falling down onto a carpet in a living room.', 44: 'Someone holding head in pain, indicating a headache, in an office.', 45: 'An individual clutching chest in pain in a home living area.', 46: 'A person holding lower back in pain in a furniture store.', 47: 'Someone holding neck in pain in a home study.', 48: 'An individual feeling nauseous, about to vomit, in a bathroom.', 49: 'A person fanning self with a magazine in a warm room.', 50: 'Someone punching the air in a boxing gym.', 51: 'An individual kicking a pillow in a bedroom.', 52: 'A person pushing a chair in a dining room.', 53: 'Someone patting a friend on the back in a coffee shop.', 54: 'An individual pointing a finger at a computer screen in an office.', 55: 'A person hugging a friend in a living room.', 56: 'Someone giving a pen to another person in an office.', 57: 'An individual touching the pocket of their jeans in a bedroom.', 58: 'Two people shaking hands in a business meeting room.', 59: 'A person walking towards a window in a bright room.', 60: 'Two individuals walking apart in a hallway of an office building.', 61: 'A person putting on headphones in a quiet study room', 62: 'An individual taking off headphones in a home office with a computer.', 63: 'Someone shooting a basketball towards a hoop in an outdoor court during sunset.', 64: 'A person bouncing a ball on a paved driveway with a basketball hoop in the background.', 65: 'An individual swinging a tennis racket at a yellow ball on a sunny tennis court.', 66: 'Someone juggling table tennis balls in a bright game room with a ping pong table.', 67: \"A person gesturing 'hush' with a finger on lips in a library filled with bookshelves.\", 68: 'An individual flicking their long hair back in a mirror reflection in a dance studio.', 69: 'A person giving a thumbs up in a bright classroom with students and a chalkboard.', 70: 'Someone giving a thumbs down in a meeting room with a large monitor displaying data.', 71: 'An individual making an OK sign with their hand in a cozy cafe with coffee cups on tables.', 72: 'A person making a victory sign with their fingers in front of a scenic viewpoint overlooking mountains.', 73: 'Someone stapling pages of a book in a crafting room with art supplies on shelves.', 74: 'An individual counting money on a wooden table in a small business office.', 75: 'A person cutting nails sitting on a porch with a garden view.', 76: 'Someone cutting paper with scissors on a cluttered craft table in an art studio.', 77: 'An individual snapping fingers to music in a brightly lit kitchen while cooking.', 78: 'A person opening a bottle in a picnic setting with a basket and a blanket.', 79: 'Someone sniffing a perfume bottle in a boutique with shelves of beauty products.', 80: 'An individual squatting down to tie shoelaces in a gym with workout equipment.', 81: 'A person tossing a coin into a fountain outdoors with trees in the background.', 82: \"Someone folding paper into an airplane in a classroom with kids' drawings on the walls.\", 83: 'An individual balling up paper in frustration in an office with a computer and documents.', 84: 'A person playing with a magic cube in a cozy living room on a soft rug.', 85: 'Someone applying cream on their face in a bright bathroom with a large mirror.', 86: 'An individual applying cream on the back of their hand in a beauty salon with products on display.', 87: 'A person putting on a backpack in a hostel dormitory with bunk beds and lockers.', 88: 'Someone taking off a backpack at the entrance of a hiking trail with woods in the background.', 89: 'An individual putting something into a bag on a kitchen counter with groceries around.', 90: 'A person taking something out of a bag in a classroom with desks and a projector.', 91: 'Someone opening a box in a living room on Christmas morning with decorations.', 92: 'An individual moving heavy objects in a garage filled with tools and storage boxes.', 93: 'A person shaking their fist in excitement at a sports event with a crowd cheering.', 94: 'Someone throwing up a cap in celebration during a graduation ceremony in an open field.', 95: 'An individual with hands up in surrender during a playful water fight in a backyard.', 96: 'A person crossing their arms while waiting in a coffee shop with a line of customers.', 97: 'Someone doing arm circles as part of a warm-up in a fitness class at a gym.', 98: 'An individual performing arm swings in a park with autumn leaves on the ground.', 99: 'A person running on the spot in a home gym with a treadmill and weights.', 100: 'Someone doing butt kicks during a track workout on a sunny day at an athletic field.', 101: 'An individual performing a cross toe touch in a yoga studio with mats and calming decor.', 102: 'A person executing a side kick in a martial arts dojo with mirrors and training pads.', 103: 'Someone yawning widely in a cozy bedroom early in the morning with the sun rising.', 104: 'An individual stretching themselves in an office during a break with a coffee cup on the desk.', 105: 'A person blowing their nose with a tissue in a bright, airy living room with a large window.', 106: 'Someone hitting another person with a foam bat in a playful outdoor party setting.', 107: 'An individual wielding a knife towards another person in a dramatic theater rehearsal scene.', 108: 'A person knocking over another person during a friendly beach volleyball game.', 109: 'Someone grabbing another person’s hat in a playful manner at a sunny park.', 110: 'An individual shooting at another person with a water gun during a summer backyard party.', 111: 'A person stepping on someone’s foot accidentally in a crowded subway car.', 112: 'Someone high-fiving in a sports team huddle on a field with goals in the background.', 113: 'An individual cheering and drinking with friends at a rooftop bar with city lights.', 114: 'Two people carrying a couch together into a new apartment with boxes around.', 115: 'A person taking a photo of another person in front of a famous landmark during a trip.', 116: 'Someone following another person in a vast open room.', 117: 'An individual whispering in another person’s ear during a secret exchange in a library.', 118: 'Two people exchanging things in an industrial building.', 119: 'Someone supporting somebody with a hand during a difficult hiking trail with scenic views.', 120: 'Two individuals playing rock-paper-scissors in a schoolyard with children watching.'}\n"
     ]
    }
   ],
   "source": [
    "class_index_to_text_description = {index: description for index, description in enumerate(text_descriptions120, 1)}\n",
    "print(class_index_to_text_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Path to the RGB videos directory\n",
    "data_root = \"/net/polaris/storage/deeplearning/ntu\"\n",
    "rgb_modality = 'nturgb+d_rgb'\n",
    "annotation_list = []\n",
    "\n",
    "# Directory containing RGB modality\n",
    "rgb_path = os.path.join(data_root, rgb_modality)\n",
    "for filename in os.listdir(rgb_path):\n",
    "    # Extract the common prefix and the action label from the filename\n",
    "    prefix, _ = os.path.splitext(filename)\n",
    "    action_label = prefix.split('A')[-1].split('_')[0]\n",
    "\n",
    "    # Convert to integer to remove leading zeros, then to string if your keys are strings\n",
    "    action_label = int(action_label)\n",
    "    # Use the action label to get the text description\n",
    "    text_description = class_index_to_text_description.get(action_label, \"Unknown action\")\n",
    "\n",
    "    # Create the annotation entry\n",
    "    annotation_entry = {\n",
    "        'clip_id': prefix,  # Common prefix as the clip ID\n",
    "        'text': text_description\n",
    "    }\n",
    "\n",
    "    # Add the annotation entry to the list\n",
    "    annotation_list.append(annotation_entry)\n",
    "\n",
    "# Save the annotations to a JSONL file\n",
    "annotation_file = \"/home/bas06400/Thesis/VIP/src/developmentANDtest/annotations_rgb_words.jsonl\"\n",
    "with open(annotation_file, 'w') as f:\n",
    "    for annotation in annotation_list:\n",
    "        f.write(json.dumps(annotation) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Cross Subject60 split\n",
    "\n",
    "# Function to extract the subject ID from the file path\n",
    "def get_subject_id(file_path):\n",
    "    start = file_path.find('P') + 2\n",
    "    end = file_path.find('R', start)\n",
    "    return file_path[start:end]\n",
    "\n",
    "# Splitting the dataset for cross-subject evaluation\n",
    "with open('/home/bas06400/Thesis/CLIPVIP_Datasets/ntu_all_generated_descriptions.jsonl', 'r') as infile, \\\n",
    "     open('/home/bas06400/Thesis/CLIPVIP_Datasets/generated_descriptions_CS_train.jsonl', 'w') as train_file, \\\n",
    "     open('/home/bas06400/Thesis/CLIPVIP_Datasets/generated_descriptions_CS_val.jsonl', 'w') as val_file:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        # Assuming the subject ID is in the second file path in the entry\n",
    "        file_paths = entry[\"clip_id\"]\n",
    "        subject_id = get_subject_id(file_paths)  # Zero-pad for consistency\n",
    "        \n",
    "\n",
    "        if subject_id in {'01', '02', '04', '05', '08', '09', '13', '14', '15', '16', '17', '18', '19', '25', '27', '28', '31', '34', '35', '38'}:\n",
    "            train_file.write(json.dumps(entry) + \"\\n\")\n",
    "        else:\n",
    "            val_file.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Cross Subject120 split\n",
    "\n",
    "# Define the training subject IDs\n",
    "training_subjects = {'001', '002', '004', '005', '008', '009', '013', '014', '015', '016', '017', '018', '019', '025', '027', '028', '031', '034', '035', '038', '045', \n",
    "                    '046', '047', '049', '050', '052', '053', '054', '055', '056', '057', '058', '059', '070', '074', '078', '080', '081', '082', '083', '084', \n",
    "                    '085', '086', '089', '091', '092', '093', '094', '095', '097', '098', '100', '103'}\n",
    "\n",
    "# Function to extract the subject ID from the file path\n",
    "def get_subject_id(file_path):\n",
    "    start = file_path.find('P') + 1\n",
    "    end = file_path.find('R', start)\n",
    "    return file_path[start:end]\n",
    "\n",
    "# Splitting the dataset for cross-subject evaluation\n",
    "with open('/home/bas06400/annotations_rgb_120.jsonl', 'r') as infile, \\\n",
    "     open('/home/bas06400/Thesis/CLIPVIP_Datasets/CS120_training_set.jsonl', 'w') as train_file, \\\n",
    "     open('/home/bas06400/Thesis/CLIPVIP_Datasets/CS120_testing_set.jsonl', 'w') as val_file:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        # Assuming the subject ID is in the second file path in the entry\n",
    "        file_paths = entry[\"clip_id\"]\n",
    "        subject_id = get_subject_id(file_paths)  # Zero-pad for consistency\n",
    "\n",
    "        if subject_id in training_subjects:\n",
    "            train_file.write(json.dumps(entry) + \"\\n\")\n",
    "        else:\n",
    "            val_file.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract the camera ID from the clip_id\n",
    "def get_camera_id(clip_id):\n",
    "    start = clip_id.find('C') + 1\n",
    "    end = clip_id.find('P', start)\n",
    "    return clip_id[start:end]\n",
    "\n",
    "# Splitting the dataset for cross-view evaluation\n",
    "with open('/home/bas06400/Thesis/VIP/src/developmentANDtest/annotations_rgb_words.jsonl', 'r') as infile, \\\n",
    "     open('annotations_rgb_comp_CV_training_set_words.jsonl', 'w') as train_file, \\\n",
    "     open('annotations_rgb_comp_CV_testing_set_words.jsonl', 'w') as val_file:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        clip_id = entry['clip_id']\n",
    "        camera_id = get_camera_id(clip_id).zfill(3)  # Zero-pad for consistency\n",
    "\n",
    "        if camera_id in {'002', '003'}:\n",
    "            train_file.write(json.dumps(entry) + \"\\n\")\n",
    "        elif camera_id == '001':\n",
    "            val_file.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract the camera ID from the clip_id\n",
    "def get_setup_id(clip_id):\n",
    "    start = clip_id.find('S') + 1\n",
    "    end = clip_id.find('C', start)\n",
    "    return clip_id[start:end]\n",
    "\n",
    "# Splitting the dataset for cross-view evaluation\n",
    "with open('/home/bas06400/annotations_rgb_120.jsonl', 'r') as infile, \\\n",
    "     open('annotations_rgb_comp_CV_training_120set.jsonl', 'w') as train_file, \\\n",
    "     open('annotations_rgb_comp_CV_testing_120set.jsonl', 'w') as val_file:\n",
    "\n",
    "    for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            clip_id = entry['clip_id']\n",
    "            setup_id = int(get_setup_id(clip_id).zfill(3))  # Zero-pad for consistency\n",
    "\n",
    "            if setup_id % 2 == 0:  # Even setup IDs for training\n",
    "                train_file.write(line)\n",
    "            else:  # Odd setup IDs for testing\n",
    "                val_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_dataset(annotation_file, train_file, val_file, test_file, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "    # Read annotations\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = [json.loads(line) for line in f]\n",
    "\n",
    "    # Group annotations by class\n",
    "    class_to_annotations = defaultdict(list)\n",
    "    for annotation in annotations:\n",
    "        class_to_annotations[annotation['text']].append(annotation)\n",
    "\n",
    "    # Shuffle annotations within each class\n",
    "    random.seed(seed)\n",
    "    for annotations in class_to_annotations.values():\n",
    "        random.shuffle(annotations)\n",
    "\n",
    "    # Split annotations for each class\n",
    "    train_annotations = []\n",
    "    val_annotations = []\n",
    "    test_annotations = []\n",
    "\n",
    "    for class_annotations in class_to_annotations.values():\n",
    "        n_total = len(class_annotations)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "\n",
    "        train_annotations.extend(class_annotations[:n_train])\n",
    "        val_annotations.extend(class_annotations[n_train:n_train + n_val])\n",
    "        test_annotations.extend(class_annotations[n_train + n_val:])\n",
    "\n",
    "    # Write splits to separate files\n",
    "    for split_annotations, output_file in zip(\n",
    "        [train_annotations, val_annotations, test_annotations],\n",
    "        [train_file, val_file, test_file]\n",
    "    ):\n",
    "        with open(output_file, 'w') as f:\n",
    "            for annotation in split_annotations:\n",
    "                f.write(json.dumps(annotation) + '\\n')\n",
    "\n",
    "# Paths to the output files\n",
    "data_root = \"/home/bas06400/ntu\"\n",
    "annotation_file = os.path.join(data_root, \"annotations_rgb.jsonl\")\n",
    "train_file = os.path.join(data_root, \"annotations_train.jsonl\")\n",
    "val_file = os.path.join(data_root, \"annotations_val.jsonl\")\n",
    "test_file = os.path.join(data_root, \"annotations_test.jsonl\")\n",
    "\n",
    "# Execute the split\n",
    "split_dataset(annotation_file, train_file, val_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'horovod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/bas06400/Thesis/CliP-ViP.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mVIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_video_retrieval\u001b[39;00m \u001b[39mimport\u001b[39;00m HDVILAVideoRetrievalDataset\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cfg \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mtrain_datasets\u001b[39m\u001b[39m\"\u001b[39m: \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mdummy_data\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsirius-a/home/bas06400/Thesis/CliP-ViP.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m vis_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/bas06400/ntu/nturgb+d_rgb\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/Thesis/VIP/src/datasets/dataset_video_retrieval.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mVIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbasic_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m flat_list_of_lists\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mVIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m mask_batch_text_tokens, img_collate\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mVIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m init_transform_dict, init_transform_dict_simple\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdecord\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdecord\u001b[39;00m \u001b[39mimport\u001b[39;00m VideoReader\n",
      "File \u001b[0;32m~/Thesis/VIP/src/datasets/dataloader.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mVIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m any_broadcast\n\u001b[1;32m     15\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMetaLoader\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" wraps multiple data loader \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/VIP/src/utils/distributed.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhorovod\u001b[39;00m \u001b[39mimport\u001b[39;00m torch \u001b[39mas\u001b[39;00m hvd\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_reduce_and_rescale_tensors\u001b[39m(tensors, rescale_denom):\n\u001b[1;32m     17\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"All-reduce and rescale tensors at once (as a flattened tensor)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m        tensors: list of Tensors to all-reduce\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m        rescale_denom: denominator for rescaling summed Tensors\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'horovod'"
     ]
    }
   ],
   "source": [
    "from VIP.src.datasets.dataset_video_retrieval import HDVILAVideoRetrievalDataset\n",
    "cfg = {\n",
    "  \"train_datasets\": \n",
    "    {\n",
    "      \"name\": \"msrvtt-9k\",\n",
    "      \"vis_format\": \"video\",\n",
    "      \"txt\": \"clip_data/vis_db/msrvtt_video_clips/train9k.jsonl\",\n",
    "      \"vis\": \"clip_data/vis_db/msrvtt_video_clips/videos_6fps\"\n",
    "    },\n",
    "  \"val_datasets\": [\n",
    "\n",
    "    {\n",
    "      \"name\": \"msrvtt-1ka\",\n",
    "      \"vis_format\": \"video\",\n",
    "      \"txt\": \"clip_data/vis_db/msrvtt_video_clips/test1ka.jsonl\",\n",
    "      \"vis\": \"clip_data/vis_db/msrvtt_video_clips/videos_6fps\"\n",
    "    }\n",
    "  ],\n",
    "  \"inference_datasets\": [\n",
    "    {\n",
    "      \"name\": \"msrvtt-1ka\",\n",
    "      \"vis_format\": \"video\",\n",
    "      \"txt\": \"clip_data/vis_db/msrvtt_video_clips/test1ka.jsonl\",\n",
    "      \"vis\": \"clip_data/vis_db/msrvtt_video_clips/videos_6fps\"\n",
    "    }\n",
    "  ],\n",
    "\n",
    "  \"train_n_clips\": 1,\n",
    "  \"train_num_frms\": 12,\n",
    "  \"test_n_clips\": 1,\n",
    "  \"test_num_frms\": 12,\n",
    "  \"sample_rate\": 0,\n",
    "  \"sample_jitter\": 1,\n",
    "  \"video_res\": [240, 320],\n",
    "  \"input_res\": [224, 224],\n",
    "  \"max_txt_len\": 50,\n",
    "\n",
    "  \"e2e_weights_path\": \"path/to/CLIP-ViP-B/16/checkpoint\",\n",
    "  \"clip_weights\": \"openai/clip-vit-base-patch16\",\n",
    "  \"clip_config\": \"openai/clip-vit-base-patch16\",\n",
    "  \"clip_vision_additional_config\": {\n",
    "      \"type\": \"ViP\",\n",
    "      \"temporal_size\": 12,\n",
    "      \"if_use_temporal_embed\": 1,\n",
    "      \"logit_scale_init_value\": 4.60,\n",
    "      \"add_cls_num\": 3\n",
    "  },\n",
    "\n",
    "  \"train_batch_size\": 16,\n",
    "  \"test_batch_size\": 16,\n",
    "  \"max_n_example_per_group\": 1,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"n_workers\": 8,\n",
    "  \"pin_mem\": 1,\n",
    "  \"fp16\": 1,\n",
    "  \"amp_level\": \"O2\",\n",
    "  \"seed\": 42,\n",
    "\n",
    "  \"optim\": \"adamw\",\n",
    "  \"betas\": [0.9, 0.98],\n",
    "  \"learning_rate\": 1e-6,\n",
    "  \"weight_decay\": 0.2,\n",
    "  \"lr_mul\": 1,\n",
    "  \"lr_mul_prefix\": \"\",\n",
    "  \"loss_config\": {\n",
    "    \"loss_name\": \"NCELearnableTempLoss\",\n",
    "    \"if_gather\": 1\n",
    "  },\n",
    "  \"warmup_ratio\": 0.01,\n",
    "  \"decay\": \"cosine\",\n",
    "  \"grad_norm\": 1.0,\n",
    "\n",
    "  \"num_train_epochs\": 100,\n",
    "  \"min_valid_steps\": 1,\n",
    "  \"num_valid\": 1,\n",
    "  \"only_valid_steps\": 100,\n",
    "  \"save_steps_ratio\": 0.9,\n",
    "  \"output_dir\": \"vidclip_data/output/msrvtt_retrieval/msrvtt_retrieval_vip_base_16\",\n",
    "  \"if_tb_log\": 0,\n",
    "  \"if_model_saver\": 1,\n",
    "  \"if_log2file\": 1,\n",
    "  \"dummy_data\": 0\n",
    "}\n",
    "\n",
    "\n",
    "vis_dir = '/home/bas06400/ntu/nturgb+d_rgb'\n",
    "anno_path = 'ntu/annotations_train.jsonl'\n",
    "\n",
    "dataset = HDVILAVideoRetrievalDataset(cfg, vis_dir, anno_path, vis_format='video', mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SingleFrameVideoDataset' from 'multimodal_dataset' (/home/bas06400/Thesis/VIP/src/multimodal_dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/bas06400/Thesis/VIP/src/CliP-ViP.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bquaoar/home/bas06400/Thesis/VIP/src/CliP-ViP.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultimodal_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m SingleFrameVideoDataset\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquaoar/home/bas06400/Thesis/VIP/src/CliP-ViP.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Set the seed for reproducibility\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bquaoar/home/bas06400/Thesis/VIP/src/CliP-ViP.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SingleFrameVideoDataset' from 'multimodal_dataset' (/home/bas06400/Thesis/VIP/src/multimodal_dataset.py)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_remote_kernel",
   "language": "python",
   "name": "my_remote_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
