{
    "task": "2",
    "topic": "llava_full_size_cropping08-04",
    "align_pre_training": false,
    "feature_save_dir": "",
    "number_gpus": "1",
    "modalities": [
        "rgb",
        "rgb2"
    ],
    "encoder_model": "MIX",
    "modalities_encoders": {
        "rgb": "CLIP-VIP",
        "rgb2": "OMNIVORE"
    },
    "dataset": "NTU",
    "split": "CV",
    "num_classes": 60,
    "in_features": 512,
    "fusion": false,
    "full_train_classifiers": false,
    "epochs": 500,
    "res_cktp": false,
    "cktp_dir": "/home/bas06400/Thesis/VIP/src/align_checkpoints",
    "aligned_model": "checkpoint_rgb_rgb2_MIX_NTU_CV_20240524_025543.pth",
    "learning_rate": 0.0001,
    "temperature": 0.1,
    "num_workers": 5,
    "data_list": "/home/bas06400/Thesis/rgb_ir_dataset.txt",
    "data_root": "/net/polaris/storage/deeplearning/ntu",
    "batch_size": 32,
    "random_sample": false,
    "pin_memory": true,
    "clip_config": "openai/clip-vit-base-patch16",
    "clip_weights": "openai/clip-vit-base-patch16",
    "clip_vision_additional_config": {
        "type": "ViP",
        "temporal_size": 12,
        "if_use_temporal_embed": true,
        "logit_scale_init_value": 4.6,
        "add_cls_num": 3
    },
    "e2e_weights_path": "/home/bas06400/Thesis/pretrain_clipvip_base_16.pt"
}